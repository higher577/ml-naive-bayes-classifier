{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6dc220",
   "metadata": {},
   "source": [
    "Implement from scratch the Naive Bayes algorithm. Then improve the model obtained using standard Naive Bayes algorithm; The improvement can include: adding new features such as n-grams (phrases of n words, for some n that is tunable hyperparameter). \n",
    "(in Python using Jupyter notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9305f5",
   "metadata": {},
   "source": [
    "**Report**\n",
    "\n",
    "**1. explain and motivate the chosen representation & data preprocessing**\n",
    "\n",
    "The dataset contains project descriptions labeled with four categories: Web Development (W), Game Development (G), Security (S), and AI/ML (A).\n",
    "\n",
    "Each description was lowercased and split into individual words. Stop words (like “the”, “and”, “is”, \"we\") were removed to reduce noise. The initial model used only unigrams (single words) to represent the text.\n",
    "\n",
    "However, Naive Bayes assumes that words appear independently, which is often unrealistic in natural language. To improve the model, extended the model to use bigrams—pairs of adjacent words (like “web development” or “decision making”). This helps capture more contextual meaning.\n",
    "\n",
    "Applied Laplace smoothing to avoid assigning zero probabilities to words or phrases not seen in the training data.\n",
    "\n",
    "Also noted that the training data is imbalanced. Over half the labels are ‘W’ (52%), while only 3% are ‘S’. This makes the model biased toward predicting more common labels.\n",
    "\n",
    "\n",
    "**2. explain the idea behind the model improvements and their implementation (including the implementation of the standard Naive Bayes)**\n",
    "\n",
    "The standard model is a basic Naive Bayes classifier. It calculates:\n",
    " - Prior probabilities: how often each label appears (e.g., P(W), P(S))\n",
    " - Conditional probabilities: how often each word appears given a label (e.g., P(word | W))\n",
    "\n",
    "To improve the model:\n",
    " - Laplace smoothing: This helps when the model sees new words or phrases it didn’t learn during training. Instead of giving them zero probability, Laplace smoothing gives them a small value.\n",
    " - N-grams (bigrams): Instead of using only single words, added bigrams to help the model learn word pairs. This helps it understand common phrases used in project descriptions.\n",
    " - Also used log probabilities when multiplying values, to avoid very small numbers (underflow problems) during calculation.\n",
    "\n",
    "\n",
    "**3. explain the evaluation procedure (e.g., cross-validation or training/validation split)**\n",
    "\n",
    "Splitted the dataset into two parts: one for training and one for testing. Trained the model using the training set and then checked how well it worked on the testing set.\n",
    "\n",
    "Accuracy was used as the main score to compare different versions of the model: Standard Naive Bayes, Naive Bayes with Laplace smoothing, Naive Bayes with n-gram features (bigrams) and Naive Bayes with n-gram with Laplace smoothing\n",
    "\n",
    "\n",
    "**4. include and explain the training/validation results for the standard and improved Naive Bayes model. You can summarize results using tables (or plots), but all results have to be explained descriptively as well.**\n",
    "\n",
    "The accuracy of each model:\n",
    "Standard Naive Bayes:\t0.91\n",
    "Standard Naive Bayes with Laplace:\t0.90\n",
    "Naive Bayes with bigrams:\t0.885\n",
    "Naive Bayes with bigrams with Laplace: 0.92\n",
    "\n",
    "The best result came from using bigrams combined with Laplace smoothing, giving an accuracy of 0.92. \n",
    "This suggests that capturing word pairs and handling unseen phrases helps the model make better predictions.\n",
    "\n",
    "\n",
    "**5. be written in plain English and should not be longer than two A4 pages (export the notebook as pdf to see if the report section fits in two pages).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "17cc3154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\higher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import sklearn \n",
    "import math\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) #common words as 'a', 'we', 'the'\n",
    "#stop_words\n",
    "\n",
    "training_data = pd.read_csv(\"train.csv\")\n",
    "#training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0c29ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Artificial Intelligence and Machine Learning (A)\n",
    "#Privacy and Security (S)\n",
    "#Game Development (G)\n",
    "#Web Development (W)\n",
    "\n",
    "#separate rows for each class, only includes description\n",
    "wd = []\n",
    "gd = []\n",
    "ps = []\n",
    "aiml = []\n",
    "for i in range(len(training_data)):\n",
    "    if (training_data[\"Class\"][i] == \"W\"):\n",
    "        wd.append(training_data[\"Description\"][i])\n",
    "    if (training_data[\"Class\"][i] == \"G\"):\n",
    "        gd.append(training_data[\"Description\"][i])\n",
    "    if (training_data[\"Class\"][i] == \"S\"):\n",
    "        ps.append(training_data[\"Description\"][i])\n",
    "    if (training_data[\"Class\"][i] == \"A\"):\n",
    "        aiml.append(training_data[\"Description\"][i])\n",
    "#wd\n",
    "\n",
    "pwd = len(wd) / len(training_data) #prior probabilities = number of label W in the dataset / all rows in the dataset\n",
    "pgd = len(gd) / len(training_data)\n",
    "pps = len(ps) / len(training_data)\n",
    "paiml = len(aiml) / len(training_data)\n",
    "#pwd\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() #all into lowercase\n",
    "    words = text.split() #split word for word\n",
    "    filtered_words = [word for word in words if word not in stop_words] #remove stop words\n",
    "    return filtered_words\n",
    "#preprocess(wd[0])\n",
    "\n",
    "\n",
    "word_by_label = {'W': {}, 'G': {}, 'S': {}, 'A': {}} #dictionary to store word counts for each label\n",
    "for label in word_by_label: #for each label\n",
    "\n",
    "    description_list = []\n",
    "    if label == 'W':\n",
    "        description_list = wd\n",
    "    if label == 'G':\n",
    "        description_list = gd\n",
    "    if label == 'S':\n",
    "        description_list = ps\n",
    "    if label == 'A':\n",
    "        description_list = aiml\n",
    "\n",
    "    for row in description_list: #for each row in description_list\n",
    "        words = preprocess(row) #preprocess each row of text\n",
    "        for word in words: #for each words,  \n",
    "            if word in word_by_label[label]: #if already exist in the dictionary, increment count\n",
    "                word_by_label[label][word] += 1\n",
    "            else: #if new unique word, add to dictionary with count 1\n",
    "                word_by_label[label][word] = 1\n",
    "#word_by_label['W'] #number of each word given label = 'W'\n",
    "#word_by_label['A']['developed']\n",
    "#len(word_by_label['W']) #number of total words given W\n",
    "\n",
    "\n",
    "\n",
    "all_words = {} #dictionary to store word counts for each label\n",
    "all_description_list = training_data['Description'] #sample size description list\n",
    "\n",
    "for row in all_description_list: #for all rows in description column\n",
    "    words = preprocess(row) #preprocess each row of text\n",
    "    for word in words: #for each words,  \n",
    "        if word in all_words: #if (key) already exist in the dictionary, increment count\n",
    "            all_words[word] += 1\n",
    "        else: #if new unique word, add (key) to dictionary with count 1\n",
    "            all_words[word] = 1\n",
    "#all_words #number of all words of the dataset\n",
    "#all_words['developed'] #number of all 'developed' total (sample size)\n",
    "\n",
    "tot_unique_words = len(all_words) #total unique words of all total (sample size)\n",
    "#tot_unique_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2e35e97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data conditional probabilities\n",
    "#we use the training data numbers to calculate the labels for the testing data\n",
    "\n",
    "#calculate for each label \n",
    "#for each row of test data\n",
    "#and choose the largest probability\n",
    "\n",
    "testing_data = pd.read_csv(\"test.csv\")\n",
    "#testing_data\n",
    "\n",
    "\n",
    "labels = [] #predicted labels \n",
    "labels_laplace = []\n",
    "for row in testing_data['Description']: #for each Description row in testing data\n",
    "\n",
    "    probabilities_by_label = {'W': {}, 'G': {}, 'S': {}, 'A': {}} #dictionary to store conditional probabilites for each label (standard Naive Bayes algorithm)\n",
    "    probabilities_by_label_laplace = {'W': {}, 'G': {}, 'S': {}, 'A': {}} #laplace smoothing\n",
    "    for label in word_by_label: #for each label\n",
    "\n",
    "        posterior_probabilites = 0\n",
    "        posterior_probabilites_laplace = 0\n",
    "        if label == 'W':\n",
    "            posterior_probabilites = pwd\n",
    "            posterior_probabilites_laplace = pwd\n",
    "        if label == 'G':\n",
    "            posterior_probabilites = pgd\n",
    "            posterior_probabilites_laplace = pgd\n",
    "        if label == 'S':\n",
    "            posterior_probabilites = pps\n",
    "            posterior_probabilites_laplace = pps\n",
    "        if label == 'A':\n",
    "            posterior_probabilites = paiml\n",
    "            posterior_probabilites_laplace = paiml\n",
    "\n",
    "        words = preprocess(row)\n",
    "        for word in words:\n",
    "            if word in word_by_label[label]: #if the word exists in the training data\n",
    "                p_word_given_label = word_by_label[label][word] / len(word_by_label[label]) #number of all 'word' given label / number of total words of the given\n",
    "                p_word_given_label_laplace = ((word_by_label[label][word]) + 1) / (len(word_by_label[label]) + tot_unique_words) #laplace smoothing\n",
    "\n",
    "            posterior_probabilites = posterior_probabilites * p_word_given_label \n",
    "            posterior_probabilites_laplace = posterior_probabilites_laplace * p_word_given_label_laplace\n",
    "        probabilities_by_label[label] = posterior_probabilites\n",
    "        probabilities_by_label_laplace[label] = posterior_probabilites_laplace\n",
    "\n",
    "        #so there will be 8 probabilities for each row of test data (4 for without smoothing, and 4 for with laplace smoothing)\n",
    "    max_label = max(probabilities_by_label, key=probabilities_by_label.get) #get the largest probability out of the labels\n",
    "    #max_value = probabilities_by_label[max_word]\n",
    "    labels.append(max_label)\n",
    "\n",
    "    max_label_laplace = max(probabilities_by_label_laplace, key=probabilities_by_label_laplace.get)\n",
    "    labels_laplace.append(max_label_laplace)\n",
    "#labels #0.91\n",
    "#labels_laplace 0.90636\n",
    "labels == labels_laplace #not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fa8cf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Id': range(1, len(labels) + 1),\n",
    "    'content': labels\n",
    "})\n",
    "# Save to CSV\n",
    "df.to_csv('output.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Id': range(1, len(labels_laplace) + 1),\n",
    "    'content': labels_laplace\n",
    "})\n",
    "# Save to CSV\n",
    "df.to_csv('output_laplace.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "bf991360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def preprocess_with_ngrams(text, n):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    ngram_tokens = ['_'.join(gram) for gram in ngrams(filtered_words, n)]  #join words e.g. ['web_development', 'development_using']\n",
    "    return filtered_words + ngram_tokens  # include both unigrams and bigrams\n",
    "#preprocess_with_ngrams(testing_data['Description'][0], n=2)\n",
    "\n",
    "\n",
    "# Count all unique unigrams + n-grams from training data\n",
    "all_words_ngrams = {}\n",
    "for row in all_description_list:\n",
    "    words = preprocess_with_ngrams(row, n=4)\n",
    "    for word in words: \n",
    "        if word in all_words_ngrams: \n",
    "            all_words_ngrams[word] += 1\n",
    "        else:\n",
    "            all_words_ngrams[word] = 1\n",
    "\n",
    "tot_unique_words_ngram = len(all_words_ngrams)\n",
    "total_vocab_size_ngram = tot_unique_words + tot_unique_words_ngram  # <-- Laplace fix\n",
    "#total_vocab_size_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "37b1da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ngrams = []\n",
    "labels_laplace_ngrams = []\n",
    "\n",
    "for row in testing_data['Description']:\n",
    "    p_by_label = {'W': 0, 'G': 0, 'S': 0, 'A': 0}\n",
    "    p_by_label_laplace = {'W': 0, 'G': 0, 'S': 0, 'A': 0}\n",
    "\n",
    "    words = preprocess_with_ngrams(row, n=2)\n",
    "\n",
    "    for label in word_by_label:\n",
    "        if label == 'W':\n",
    "            pp = math.log(pwd)\n",
    "        if label == 'G':\n",
    "            pp = math.log(pgd)\n",
    "        if label == 'S':\n",
    "            pp = math.log(pps)\n",
    "        if label == 'A':\n",
    "            pp = math.log(paiml)\n",
    "\n",
    "        pp_laplace = pp\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_by_label[label]:\n",
    "                p_word_given_label = word_by_label[label][word] / len(word_by_label[label])\n",
    "                pp += math.log(p_word_given_label)\n",
    "\n",
    "            count = word_by_label[label].get(word, 0)\n",
    "            prob = (count + 1) / (len(word_by_label[label]) + total_vocab_size_ngram)\n",
    "            pp_laplace += math.log(prob)\n",
    "\n",
    "        p_by_label[label] = pp\n",
    "        p_by_label_laplace[label] = pp_laplace\n",
    "\n",
    "    max_label_ngrams = max(p_by_label, key=p_by_label.get)\n",
    "    labels_ngrams.append(max_label_ngrams)\n",
    "\n",
    "    max_label_laplace = max(p_by_label_laplace, key=p_by_label_laplace.get)\n",
    "    labels_laplace_ngrams.append(max_label_laplace)\n",
    "#labels_ngrams\n",
    "#labels_laplace_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "768d8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Id': range(1, len(labels_ngrams) + 1),\n",
    "    'content': labels_ngrams\n",
    "})\n",
    "# Save to CSV\n",
    "df.to_csv('output_ngrams.csv', index=False)\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Id': range(1, len(labels_laplace_ngrams) + 1),\n",
    "    'content': labels_laplace_ngrams\n",
    "})\n",
    "# Save to CSV\n",
    "df.to_csv('output_laplace_ngrams.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
